{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a88a1e30-1980-4b6e-afb3-b682f0ce7a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6a1fc9c-b695-45d1-96f7-f6ba4d0873ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "616a390d-25af-41df-bcb7-96f4f11e5623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-23 22:02:18--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-09.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/b5af7693-2f26-4bd5-8854-75edeb650bae?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240124%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240124T040218Z&X-Amz-Expires=300&X-Amz-Signature=19f147495d43c251c3c2ed252ea44e005cd4f0787ef5ed403fca45d98bdf836d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dgreen_tripdata_2019-09.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-01-23 22:02:18--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/b5af7693-2f26-4bd5-8854-75edeb650bae?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240124%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240124T040218Z&X-Amz-Expires=300&X-Amz-Signature=19f147495d43c251c3c2ed252ea44e005cd4f0787ef5ed403fca45d98bdf836d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dgreen_tripdata_2019-09.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7854533 (7.5M) [application/octet-stream]\n",
      "Saving to: ‘green_tripdata_2019-09.csv.gz.1’\n",
      "\n",
      "green_tripdata_2019 100%[===================>]   7.49M  4.03MB/s    in 1.9s    \n",
      "\n",
      "2024-01-23 22:02:20 (4.03 MB/s) - ‘green_tripdata_2019-09.csv.gz.1’ saved [7854533/7854533]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-09.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02c9b786-6dcb-4dfb-96ca-90017c7fc4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial uses the below statement to read the taxi data, first 100 rows\n",
    "\n",
    "df = pd.read_csv('green_tripdata_2019-09.csv.gz', nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb186c90-9071-4533-83af-acfd2b543106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"green_taxi_data\" (\n",
      "\"VendorID\" INTEGER,\n",
      "  \"lpep_pickup_datetime\" TEXT,\n",
      "  \"lpep_dropoff_datetime\" TEXT,\n",
      "  \"store_and_fwd_flag\" TEXT,\n",
      "  \"RatecodeID\" INTEGER,\n",
      "  \"PULocationID\" INTEGER,\n",
      "  \"DOLocationID\" INTEGER,\n",
      "  \"passenger_count\" INTEGER,\n",
      "  \"trip_distance\" REAL,\n",
      "  \"fare_amount\" REAL,\n",
      "  \"extra\" REAL,\n",
      "  \"mta_tax\" REAL,\n",
      "  \"tip_amount\" REAL,\n",
      "  \"tolls_amount\" REAL,\n",
      "  \"ehail_fee\" REAL,\n",
      "  \"improvement_surcharge\" REAL,\n",
      "  \"total_amount\" REAL,\n",
      "  \"payment_type\" INTEGER,\n",
      "  \"trip_type\" INTEGER,\n",
      "  \"congestion_surcharge\" REAL\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pd.io.sql.get_schema(df, name = 'green_taxi_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7f9ae46-5457-4474-8460-5abb16c1502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code to get the DDL recognizes the pickup and dropoff variables as text instead of timestamp.\n",
    "# We use the following to get pandas to recognize these variables as datetime so that the DDL is correct.\n",
    "# Pandas function that takes text and parses it, creating datetime object\n",
    "\n",
    "df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaa8e906-b541-4b9c-b9d4-8d194dce7ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get DDL that will work with postgres (postgres dialect) we first need to make a connection to postgres\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c58638f-ac82-4ad4-a995-ee9ec1a55526",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e94f28b3-91f9-4636-82e8-ad3079c2b6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE green_taxi_data (\n",
      "\t\"VendorID\" BIGINT, \n",
      "\tlpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tlpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\t\"RatecodeID\" BIGINT, \n",
      "\t\"PULocationID\" BIGINT, \n",
      "\t\"DOLocationID\" BIGINT, \n",
      "\tpassenger_count BIGINT, \n",
      "\ttrip_distance FLOAT(53), \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\tehail_fee FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tpayment_type BIGINT, \n",
      "\ttrip_type BIGINT, \n",
      "\tcongestion_surcharge FLOAT(53)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we want to take this dataset and put it in postgres\n",
    "# First we have to generate a table schema that specifies what kind of columns there are, what types, etc.\n",
    "# To do this we use the following, which gives us DDl (data definition language) that specifies the schema\n",
    "\n",
    "print(pd.io.sql.get_schema(df, name = 'green_taxi_data', con = engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba572afe-b131-4722-9cca-0457b68d326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far, we only read in 100 rows of data, but there are many rows to read in. \n",
    "# The full dataframe is too big, we don't want to insert 1M+ rows of data into the database at the same time\n",
    "# We need to batch the data. To do this we will use iterators from pandas.\n",
    "# Iterators allow us to chuck the file into smaller dataframes.\n",
    "\n",
    "df_iter = pd.read_csv('green_tripdata_2019-09.csv.gz', iterator = True, chunksize = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14ccfd97-4b36-4c39-a799-b5b423d1dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next if a funtion that returns the next element in an iterator\n",
    "df = next(df_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d089ff12-c0d1-49da-987c-e7b5bcb0649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b257bb09-f5f2-44eb-a051-7b7f738dab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure variables are parsed correctly\n",
    "\n",
    "df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a18aa62-1b87-4b24-866f-8b17c3bc1e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we want to generate the DDL statement from above and create a table in the database\n",
    "# Create the table in the database, but don't insert data yet\n",
    "\n",
    "df.head(n=0).to_sql(name = 'green_taxi_data', con = engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ff5b4ee-6035-44ba-8c5f-e3ad0e802fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.02 s, sys: 92.3 ms, total: 3.11 s\n",
      "Wall time: 5.06 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we insert the data \n",
    "\n",
    "%time df.to_sql(name = 'green_taxi_data', con = engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c587dabb-7619-4f85-bcb1-03216558979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7d536f4-2ac7-40b9-825d-ec66c111cfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted another chunk..., took 5.191 second\n",
      "inserted another chunk..., took 5.170 second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/12/c7qw9tj5267_v6lqg1cnvldc0000gn/T/ipykernel_22235/3747775942.py:6: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = next(df_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted another chunk..., took 5.163 second\n",
      "inserted another chunk..., took 2.183 second\n",
      "Finished ingesting data into the postgres database\n"
     ]
    }
   ],
   "source": [
    "# Now we insert the rest of the dataframe to the table \n",
    "# '%.3f' treat as float and will have 3 decimal digits\n",
    "while True:\n",
    "    try:\n",
    "        t_start = time()\n",
    "        df = next(df_iter)\n",
    "\n",
    "        df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "        df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "\n",
    "        df.to_sql(name = 'green_taxi_data', con = engine, if_exists='append')\n",
    "\n",
    "        t_end = time()\n",
    "\n",
    "        print('inserted another chunk..., took %.3f second' % (t_end - t_start))\n",
    "\n",
    "    except StopIteration:\n",
    "            print(\"Finished ingesting data into the postgres database\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "014f8b1b-4319-484d-afc6-6f338a574edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-23 22:17:37--  https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 16.182.73.144, 54.231.132.168, 52.217.112.88, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|16.182.73.144|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi+_zone_lookup.csv.1’\n",
      "\n",
      "taxi+_zone_lookup.c 100%[===================>]  12.03K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2024-01-23 22:17:37 (139 KB/s) - ‘taxi+_zone_lookup.csv.1’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff67e111-2ab0-4ca8-a2e2-a8bf3be7156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = pd.read_csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31584bbf-7bc7-4a67-a211-eabb29354942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.to_sql(name = 'zones', con = engine, if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6e06d8-5770-4671-858f-7ee6e23638ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
